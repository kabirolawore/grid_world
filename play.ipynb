{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669d712",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# global variables\n",
    "BOARD_ROWS = 5\n",
    "BOARD_COLS = 5\n",
    "\n",
    "WIN_STATE = (4, 4)  # Adjusted to 0-based indexing\n",
    "JUMP_STATE = (1, 3)  # Adjusted to 0-based indexing\n",
    "START = (1, 0)  # Adjusted to 0-based indexing\n",
    "OBSTACLES = [(2, 2), (2, 3), (2, 4), (3, 2)]\n",
    "\n",
    "\n",
    "DETERMINISTIC = True\n",
    "token = ''\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, state=START):\n",
    "        # Initialize the state of the grid\n",
    "\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC  # Determine if actions are deterministic or stochastic\n",
    "\n",
    "    def give_reward(self):\n",
    "        # Return the reward based on the current state\n",
    "\n",
    "        if self.state == WIN_STATE:\n",
    "            return +10\n",
    "        elif self.state == JUMP_STATE:\n",
    "            return +5\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def is_end_func(self):\n",
    "        # Check if the current state is an end state\n",
    "\n",
    "        if self.state == WIN_STATE:\n",
    "            self.isEnd = True\n",
    "\n",
    "    def nxt_position(self, action):\n",
    "        # Determine the next position based on the action taken\n",
    "\n",
    "        \"\"\"\n",
    "        action: up, down, left, right, jump\n",
    "        -------------\n",
    "        0 | 1 | 2| 3| 4|\n",
    "        1 |\n",
    "        2 |\n",
    "        3 |\n",
    "        4 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        # print(\"testing self.state\", self.state)\n",
    "\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxt_state = (self.state[0] - 1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxt_state = (self.state[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxt_state = (self.state[0], self.state[1] - 1)\n",
    "            elif action == \"jump\":\n",
    "                nxt_state = (self.state[0] + 2, self.state[1])\n",
    "            else:\n",
    "                # right state\n",
    "                nxt_state = (self.state[0], self.state[1] + 1)\n",
    "\n",
    "            # Check if the next state is legal\n",
    "\n",
    "            if (nxt_state[0] >= 0) and (nxt_state[0] <= (BOARD_ROWS - 1)):  # Check row bounds for 5x5 grid\n",
    "                if (nxt_state[1] >= 0) and (nxt_state[1] <= (BOARD_COLS - 1)):  # Check column bounds for 5x5 grid\n",
    "                    if nxt_state not in OBSTACLES:  # Exclude specified positions\n",
    "                        return nxt_state\n",
    "                    \n",
    "            # Only if all conditions are met does the move succeed; otherwise, the agent does not move.\n",
    "            return self.state\n",
    "\n",
    "    def show_board(self):\n",
    "        # Display the current state of the grid\n",
    "\n",
    "        global token\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "\n",
    "# Agent of player\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Initialize the agent with states, actions, and state values\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\", \"jump\"]  # Possible actions for the agent\n",
    "        self.State = State()\n",
    "        self.lr = 0.2  # Learning rate for updating state values\n",
    "        self.exp_rate = 0.3  # Exploration rate for epsilon-greedy exploration\n",
    "\n",
    "        # Initialize state values (Q-values)\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = 0  # set initial value to 0\n",
    "\n",
    "# Help me understand line by line in simple terms what this function below does in the overall code\n",
    "    def choose_action(self):\n",
    "\n",
    "        # Choose an action based on the current state and exploration strategy\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "            while action == 'jump' and self.State.state != (1, 3):\n",
    "                action = np.random.choice(self.actions)\n",
    "\n",
    "        else:\n",
    "            # Greedy action selection\n",
    "            for a in self.actions:\n",
    "                # Only consider the next state if the action is valid from current state\n",
    "                if self.State.nxt_position(\n",
    "                        a) != self.State.state:  # Check if action is valid (doesn't stay in the same place)\n",
    "                    nxt_reward = self.state_values[self.State.nxt_position(a)]\n",
    "                    if a == \"jump\" and self.State.state != (1, 3):  # Skip jump if not at JUMP_STATE\n",
    "                        continue  # Move on to the next action in the loop\n",
    "                    if nxt_reward >= mx_nxt_reward:\n",
    "                        action = a\n",
    "                        mx_nxt_reward = nxt_reward\n",
    "\n",
    "        return action\n",
    "\n",
    "    def take_action(self, action):\n",
    "        # Take action and transition to the next state\n",
    "\n",
    "        position = self.State.nxt_position(action)\n",
    "        return State(state=position)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the agent's states for a new episode\n",
    "\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "\n",
    "    def play(self, rounds=100):\n",
    "        # Train the agent by playing multiple episodes\n",
    "\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of game back propagate reward\n",
    "            if self.State.isEnd:\n",
    "                # back propagate\n",
    "                reward = self.State.give_reward()\n",
    "\n",
    "                # explicitly assign end state to reward values\n",
    "                self.state_values[self.State.state] = reward  # this is optional\n",
    "                print(\"Game End Reward\", reward)\n",
    "\n",
    "                for s in reversed(self.states):\n",
    "                    reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
    "                    # print('reward testing', reward)\n",
    "                    self.state_values[s] = round(reward, 4)\n",
    "                    # print('state values', self.states)\n",
    "                    # print(self.state_values)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.choose_action()\n",
    "                # append trace\n",
    "                self.states.append(self.State.nxt_position(action))\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.take_action(action)\n",
    "                # mark is end\n",
    "                self.State.is_end_func()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                print(\"------------------------------------\")\n",
    "\n",
    "    def show_values(self):\n",
    "        # Display the state values\n",
    "\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------------------')\n",
    "\n",
    "    def plot_state_values(self):\n",
    "        # Plot heatmap for state values\n",
    "\n",
    "        q_values = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                q_values[i, j] = max(self.state_values[(i, j)] for a in self.actions)\n",
    "        plt.imshow(q_values, cmap='viridis', origin='upper')\n",
    "        plt.colorbar()\n",
    "        plt.title('Maximum Q-value for each state')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_state_values_2(self):\n",
    "        # Plot grid cells for state values\n",
    "\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                plt.text(j, i, str(self.state_values[(i, j)]), ha='center', va='center', fontsize=12)\n",
    "        plt.gca().set_yticks(np.arange(-0.5, BOARD_ROWS, 1), minor=True)\n",
    "        plt.gca().set_xticks(np.arange(-0.5, BOARD_COLS, 1), minor=True)\n",
    "        plt.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.title('Accumulated State Values')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    ag.play(100)\n",
    "    print(ag.show_values())\n",
    "    ag.plot_state_values_2()\n",
    "    ag.plot_state_values()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
